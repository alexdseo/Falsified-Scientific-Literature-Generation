{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8491a004",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b231af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# https://www.thepythoncode.com/article/extract-pdf-images-in-python\n",
    "import fitz # PyMuPDF\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# https://github.com/joke2k/faker\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "import csv\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df9350",
   "metadata": {},
   "source": [
    "# Load Bik data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972366ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe from file\n",
    "# Bik_v2 is updated with First Author Affiliation completely filled\n",
    "bik_df = pd.read_csv('Bik_v2.tsv', sep='\\t', encoding='unicode-escape')\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8472d",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b92181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first column (indices), since pandas automatically provides indices - column 0\n",
    "# Drop duplicate university name column - column 27\n",
    "\n",
    "bik_df = bik_df.drop(bik_df.columns[[0,27]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters from Affiliations\n",
    "# Easier for LaTeX\n",
    "\n",
    "fa_aff = bik_df[\"First Author Affiliation\"].tolist()\n",
    "fa_aff = [i.strip() if isinstance(i,str) else np.nan for i in fa_aff]\n",
    "\n",
    "for i in range(len(fa_aff)):\n",
    "    if fa_aff[i] == \"UniversitÃ\\xa0 degli Studi di Bari Aldo Moro\":\n",
    "        fa_aff[i] = \"University of Bari Aldo Moro\"\n",
    "    elif fa_aff[i] == \"Blood transfusion centre of Slovenia - Zavod Republike Slovenije za transfuzijsko medicino\":\n",
    "        fa_aff[i] = \"Blood Transfusion Centre of Slovenia\"\n",
    "    elif fa_aff[i] == \"Justus-Liebig-UniversitÃ¤t GieÃ\\x9fen\":\n",
    "        fa_aff[i] = \"Justus Liebig University Giessen\"\n",
    "    elif fa_aff[i] == \"University of Wisconsinâ\\x80\\x93Madison\":\n",
    "        fa_aff[i] = \"University of Wisconsin-Madison\"\n",
    "    elif fa_aff[i] == \"University of Iowa, Iowa City, USA\":\n",
    "        fa_aff[i] = \"University of Iowa\"\n",
    "    elif fa_aff[i] == \"Universidad PolitÃ©cnica de Valencia-C.S.I.C\":\n",
    "        fa_aff[i] = \"Technical University of Valencia\"\n",
    "    elif fa_aff[i] == \"Concordia Universityâ\\x80\\x93Ann Arbor\":\n",
    "        fa_aff[i] = \"Concordia University Ann Arbor\"\n",
    "    elif fa_aff[i] == \"University of Maryland, College Park\":\n",
    "        fa_aff[i] = \"University of Maryland\"\n",
    "    elif fa_aff[i] == \"UniversitÃ¤tsklinikum Erlangen\":\n",
    "        fa_aff[i] = \"University Hospital Erlangen\"\n",
    "    elif fa_aff[i] == \"Universidad AndrÃ©s Bello\":\n",
    "        fa_aff[i] = \"Andres Bello National University\"\n",
    "    elif fa_aff[i] == \"Hospital Son Dureta and Instituto Universitario de InvestigaciÃ³n en Ciencias de la Salud\":\n",
    "        fa_aff[i] = \"Hospital Son Dureta and Instituto Universitario de Investigacion en Ciencias de la Salud\"\n",
    "    elif fa_aff[i] == \"Department of OncologyFaculty of MedicineMcGill UniversityLady Davis Institute-Segal Cancer Center from the Jewish General HospitalMontrealQuebecCanada\":\n",
    "        fa_aff[i] = \"McGill University\"\n",
    "    elif fa_aff[i] == \"Cancer Research LaboratoryFudan University Shanghai Cancer CenterShanghai China\":\n",
    "        fa_aff[i] = \"Fudan University\"\n",
    "    elif fa_aff[i] == \"Department of Biochemistry and Molecular BiophysicsBiochemistry and Molecular and Cellular Biology Graduate ProgramUniversity of ArizonaTucson AZ USA\":\n",
    "        fa_aff[i] = \"University of Arizona\"\n",
    "    elif fa_aff[i] == \"Department of Experimental and Clinical MedicineUniversitÃ\\xa0 Politecnica delle MarcheVia Tronto 10/aAncona60020 Italy\":\n",
    "        fa_aff[i] = \"Marche Polytechnic University Faculty of Medicine\"\n",
    "    elif fa_aff[i] == \"UniversitÃ© de Strasbourg-CNRS\" or fa_aff[i] == \"CNRS UniversitÃ© de Strasbourg\":\n",
    "        fa_aff[i] = \"University of Strasbourg\"\n",
    "    elif fa_aff[i] == \"Divisiont of Urology Kobe University Graduate School of Medicine Kobe Hyogo 650-0017 Japan\":\n",
    "        fa_aff[i] = \"Kobe University\"\n",
    "    elif fa_aff[i] == \"School of Human Science and Environment University of Hyogo Himeji Hyogo 670-0092 Japan\":\n",
    "        fa_aff[i] = \"University of Hyogo\"\n",
    "    elif fa_aff[i] == \"Johns Hopkins University School of Medicine and the Sidney Kimmel Comprehensive Cancer Center at Johns Hopkins\":\n",
    "        fa_aff[i] = \"Johns Hopkins University\"\n",
    "    elif fa_aff[i] == \"UniversitÃ© Paris-Saclay\":\n",
    "        fa_aff[i] = \"University of Paris-Saclay\"\n",
    "    elif fa_aff[i] == \"INFN - Istituto Nazionale di Fisica Nuclear\":\n",
    "        fa_aff[i] = \"Istituto Nazionale di Fisica Nuclear\"\n",
    "    elif fa_aff[i] == 'UniversitÃ\\xa0 degli Studi della Campania \"Luigi Vanvitelli':\n",
    "        fa_aff[i] = \"University of Campania Luigi Vanvitelli\"\n",
    "    elif fa_aff[i] == \"UniversitÃ© CÃ´te d'Azur\":\n",
    "        fa_aff[i] = \"Cote d'Azur University\"\n",
    "    elif fa_aff[i] == \"Institut de CancÃ©rologie Gustave Roussy\":\n",
    "        fa_aff[i] = \"Gustave Roussy Institute of Cancerology\"\n",
    "    elif fa_aff[i] == \"Graduate School at Shenzhen, Tsinghua Univeristy\":\n",
    "        fa_aff[i] = \"Tsinghua University\"\n",
    "    elif fa_aff[i] == \"Ecole Normale SupÃ©rieure de Paris\":\n",
    "        fa_aff[i] = \"Ecole Normale Superieure, Paris\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bik_df = bik_df.drop(columns=[\"First Author Affiliation\"])\n",
    "bik_df.insert(22,\"First Author Affiliation\",fa_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea277bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bik_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Force error here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3609d3",
   "metadata": {},
   "source": [
    "# Download PDFs\n",
    "\n",
    "- **USC VPN required**: `Wiley` [119:121], `Cancer` [135:143], `SciDirect` [144:177], `Science` [178:180], `Nature` [181:187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed600225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary of Elsevier IDs to use later for file renaming\n",
    "elsevierID = {}\n",
    "\n",
    "# Download PDFs\n",
    "def download_pdf(df,url,indx):\n",
    "    \n",
    "    # Create PDFS folder to save PDFs to\n",
    "    folder_location = r'PDFS'\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "    \n",
    "    # Get PDF download link\n",
    "    if df[\"Home Site\"][indx] == \"PLOS\":\n",
    "        pdf_link = \"https://journals.plos.org/plosone/article/file?id=\" + df[\"DOI\"][indx] + \"&type=printable\"\n",
    "    elif df[\"Home Site\"][indx] == \"ASM\":\n",
    "        pdf_link = \"https://journals.asm.org/doi/pdf/\" + df[\"DOI\"][indx]\n",
    "    elif df[\"Home Site\"][indx] == \"PubMed\":\n",
    "        # Scrape page\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        # Get DOI\n",
    "        doi = soup.find(\"a\",{\"data-ga-action\":\"DOI\"}).text.replace(\"\\n\",\"\").strip()\n",
    "        # Replace PMID with DOI in dataframe\n",
    "        df.at[indx,'DOI'] = doi\n",
    "        # PDF download link format for ASM Journals\n",
    "        pdf_link = \"https://journals.asm.org/doi/pdf/\" + doi\n",
    "    elif df[\"Home Site\"][indx] == \"Hindawi\":\n",
    "        if df[\"Citation\"][indx][:3] == \"Bio\":\n",
    "            pdf_link = \"https://downloads.hindawi.com/journals/bmri/\" + \"/\".join(df[\"DOI\"][indx].split(\"/\")[-2:]) + \".pdf\"\n",
    "        else: # == \"Evi\"\n",
    "            pdf_link = \"https://downloads.hindawi.com/journals/ecam/\" + \"/\".join(df[\"DOI\"][indx].split(\"/\")[-2:]) + \".pdf\"\n",
    "    elif df[\"Home Site\"][indx] == \"Wiley\":\n",
    "        if df[\"DOI\"][indx][3:7] == \"1002\":\n",
    "            pdf_link = \"https://onlinelibrary.wiley.com/doi/pdf/\" + df[\"DOI\"][indx]\n",
    "        else: # == \"1111\"\n",
    "            pdf_link = \"https://sfamjournals.onlinelibrary.wiley.com/doi/pdf/\" + df[\"DOI\"][indx]\n",
    "    elif df[\"Home Site\"][indx] == \"BioMed\":\n",
    "        if df[\"DOI\"][indx][8:10] == \"gb\":\n",
    "            pdf_link = \"https://genomebiology.biomedcentral.com/track/pdf/\" + df[\"DOI\"][indx] + \".pdf\"\n",
    "        elif df[\"DOI\"][indx][8:10] == \"bc\":\n",
    "            pdf_link = \"https://breast-cancer-research.biomedcentral.com/track/pdf/\" + df[\"DOI\"][indx] + \".pdf\"\n",
    "        else: # == \"14\"\n",
    "            pdf_link = \"https://bmcmicrobiol.biomedcentral.com/track/pdf/\" + df[\"DOI\"][indx] + \".pdf\"\n",
    "    elif df[\"Home Site\"][indx] == \"Cancer\":\n",
    "        if df[\"DOI\"][indx][8:12] == \"j.cc\":\n",
    "            # Scrape page\n",
    "            s = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service = s)\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            # Choose PDF viewing option\n",
    "            pdf_options = soup.find(\"li\", class_=\"article-tools__item article-tools__pdf\").find_all(\"a\")\n",
    "            for a in pdf_options:\n",
    "                if pdf_options[0][\"href\"] == \"#\":\n",
    "                    url_path = pdf_options[1][\"href\"]\n",
    "                else:\n",
    "                    url_path = pdf_options[0][\"href\"]\n",
    "            url_domain = \"https://www.cell.com\"\n",
    "            # Combine URL domain and scraped path\n",
    "            pdf_link = url_domain + url_path\n",
    "        else: # == S016 or j.lu\n",
    "            s = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service = s)\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # Wait while URL redirects\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            pdf_link = driver.current_url.split(\"?\")[0] + \"/pdfft?isDTMRedir=true&download=true\"\n",
    "            elsevierID[indx] = str(driver.current_url.split(\"?\")[0].split(\"/\")[-1])\n",
    "    elif df[\"Home Site\"][indx] == \"SciDirect\":\n",
    "        # Scrape page\n",
    "        s = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service = s)\n",
    "        driver.get(url)\n",
    "        time.sleep(5) # Wait while URL redirects\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        pdf_link = driver.current_url.split(\"?\")[0] + \"/pdfft?isDTMRedir=true&download=true\"\n",
    "        elsevierID[indx] = str(driver.current_url.split(\"?\")[0].split(\"/\")[-1]) \n",
    "    elif df[\"Home Site\"][indx] == \"Science\":\n",
    "        pdf_link = \"https://www.science.org/doi/pdf/\" + df[\"DOI\"][indx]\n",
    "    elif df[\"Home Site\"][indx] == \"Nature\":\n",
    "        if df[\"DOI\"][indx][8:11] == \"onc\":\n",
    "            pdf_link = \"https://www.nature.com/articles/\" + df[\"DOI\"][indx].split(\"/\")[1].replace(\".\",\"\") + \".pdf\"\n",
    "        else:\n",
    "            pdf_link = \"https://www.nature.com/articles/\" + df[\"DOI\"][indx].split(\"/\")[1] + \".pdf\"\n",
    "    elif df[\"Home Site\"][indx] == \"RUPress\":\n",
    "        pdf_link = \"https://rupress.org/jcb/article-pdf/199/3/481/1357646/\" + df[\"DOI\"][indx].split(\"/\")[1].replace(\".\",\"_\") + \".pdf\"\n",
    "    elif df[\"Home Site\"][indx] == \"Spandidos\":\n",
    "        pdf_link = \"https://www.spandidos-publications.com/\" + df[\"DOI\"][indx] + \"/download\"\n",
    "    elif df[\"Home Site\"][indx] == \"PNAS\":\n",
    "        pdf_link = \"https://www.pnas.org/doi/pdf/\" + df[\"DOI\"][indx]\n",
    "    else:\n",
    "        pdf_link = str(indx) + \": ERROR - Link not recognized.\"\n",
    "    \n",
    "    # Feedback\n",
    "    print(pdf_link)\n",
    "    \n",
    "    # File name: Bik index\n",
    "    filename = folder_location + \"/\" + str(indx) + \".pdf\"\n",
    "    \n",
    "    # Download PDF from link\n",
    "    # https://stackoverflow.com/questions/54616638/download-all-pdf-files-from-a-website-using-python\n",
    "    try:\n",
    "        # Not blocked by Cloudflare\n",
    "        if df[\"Home Site\"][indx] in [\"PLOS\",\"Hindawi\",\"BioMed\",\"Cancer\",\"Nature\",\"Spandidos\"]:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(requests.get(pdf_link).content)\n",
    "        # Bypass Cloudflare\n",
    "        elif df[\"Home Site\"][indx] in [\"ASM\",\"PubMed\",\"Wiley\",\"Cancer\",\"SciDirect\",\"Science\",\"RUPress\",\"PNAS\"]:\n",
    "            s = Service(ChromeDriverManager().install())\n",
    "            # Bypass Cloudflare: https://blog.m157q.tw/posts/2020/09/11/bypass-cloudflare-detection-while-using-selenium-with-chromedriver\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            # Download PDF with Selenium: https://stackoverflow.com/questions/43149534/selenium-webdriver-how-to-download-a-pdf-file-with-python\n",
    "            options.add_experimental_option('prefs', {\n",
    "            \"download.default_directory\": \"/Users/auderoy/dsci550/Assignment 2/PDFS\", # SET FILE PATH\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"plugins.always_open_pdf_externally\": True\n",
    "            })\n",
    "            driver = webdriver.Chrome(service=s,options=options)\n",
    "            driver.get(pdf_link)\n",
    "            time.sleep(5)\n",
    "        # Bypass Cloudflare, click to confirm download\n",
    "        elif df[\"Home Site\"][indx] == \"Wiley\":\n",
    "            s = Service(ChromeDriverManager().install())\n",
    "            # Bypass Cloudflare\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            # Download PDF with Selenium\n",
    "            options.add_experimental_option('prefs', {\n",
    "            \"download.default_directory\": \"/Users/auderoy/dsci550/Assignment 2/PDFS\", # SET FILE PATH\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"plugins.always_open_pdf_externally\": True\n",
    "            })\n",
    "            driver = webdriver.Chrome(service=s,options=options)\n",
    "            driver.get(pdf_link)\n",
    "            time.sleep(5)\n",
    "            # Click button in iframe to confirm download: https://www.selenium.dev/documentation/webdriver/browser/frames\n",
    "            driver.switch_to.frame(driver.find_element(By.CSS_SELECTOR,'body.pb-ui').find_element(By.TAG_NAME,'iframe'))\n",
    "            driver.find_element(By.XPATH,'''/html/body/div/div/a/button''').click()\n",
    "            time.sleep(5)    \n",
    "    except:\n",
    "        print(\"------------------------------\")\n",
    "        print(f\"ERROR: Index - {indx}, Home Site - {df['Home Site'][indx]}, PDF link - {pdf_link}\")\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e374e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call download_pdf for each paper\n",
    "for index, row in bik_df.iterrows():\n",
    "    print(\"INDEX: \", index)\n",
    "    print(row[\"Home Site\"], \", \", row[\"DOI\"])\n",
    "    print(row[\"Title\"])\n",
    "    print(row[\"URL\"])\n",
    "    # download_pdf(bik_df,row[\"URL\"],index) # PDFs already downloaded; uncomment to redownload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efa7ec",
   "metadata": {},
   "source": [
    "## Rename PDFs downloaded with Selenium &rarr; \"{Bik index}.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee03a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary saved for convenience, derived from download_pdf\n",
    "# Matching Bik index to Elselvier ID\n",
    "elsevierID = {135: 'S0169500201002124',\n",
    " 136: 'S0169500203002393',\n",
    " 137: 'S016950020600287X',\n",
    " 138: 'S0169500206003308',\n",
    " 139: 'S0169500208003085',\n",
    " 140: 'S0169500209003687',\n",
    " 141: 'S0169500209005418',\n",
    " 142: 'S0169500211000481',\n",
    " 143: 'S0169500211005150',\n",
    " 144: 'S089684110500123X',\n",
    " 145: 'S0896841104001234',\n",
    " 146: 'S0896841106000357',\n",
    " 147: 'S0896841107000182',\n",
    " 148: 'S089684110800139X',\n",
    " 149: 'S089684111400064X',\n",
    " 150: 'S1043466603004423',\n",
    " 151: 'S1043466605000323',\n",
    " 152: 'S1043466605000128',\n",
    " 153: 'S104346660600216X',\n",
    " 154: 'S1043466607000038',\n",
    " 155: 'S1043466606003450',\n",
    " 156: 'S1043466606000020',\n",
    " 157: 'S1043466607000828',\n",
    " 158: 'S1043466608001853',\n",
    " 159: 'S1043466608001816',\n",
    " 160: 'S1043466608000021',\n",
    " 161: 'S1043466608000409',\n",
    " 162: 'S1043466609001896',\n",
    " 163: 'S1043466609000271',\n",
    " 164: 'S1043466609000325',\n",
    " 165: 'S1043466609000179',\n",
    " 166: 'S1043466609008813',\n",
    " 167: 'S1043466611001839',\n",
    " 168: 'S1043466611006806',\n",
    " 169: 'S1043466611000573',\n",
    " 170: 'S1043466610006897',\n",
    " 171: 'S1043466611001827',\n",
    " 172: 'S1043466612001597',\n",
    " 173: 'S1043466612006667',\n",
    " 174: 'S1043466612002177',\n",
    " 175: 'S1043466613001749',\n",
    " 176: 'S1043466613001531',\n",
    " 177: 'S1043466614004748'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename ASM, PubMed, Science, RUPress, PNAS papers\n",
    "# SET FILE PATH (x3)\n",
    "for f in os.listdir('/Users/auderoy/dsci550/Assignment 2/PDFS'):\n",
    "    count = 0\n",
    "    for c in bik_df[\"DOI\"]:\n",
    "        if c[12:] == f[4:-4]:\n",
    "            shutil.move(os.path.join('/Users/auderoy/dsci550/Assignment 2/PDFS',f),os.path.join('/Users/auderoy/dsci550/Assignment 2/PDFS',f\"{count}.pdf\"))\n",
    "        else:\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Elsevier (Cancer, SciDirect) papers\n",
    "# SET FILE PATH (x3)\n",
    "for f in os.listdir('/Users/auderoy/dsci550/Assignment 2/PDFS'):\n",
    "    for key,value in elsevierID.items():\n",
    "        try:\n",
    "            if value in f.split(\"-\")[2]:\n",
    "                shutil.move(os.path.join('/Users/auderoy/dsci550/Assignment 2/PDFS',f),os.path.join('/Users/auderoy/dsci550/Assignment 2/PDFS',f\"{key}.pdf\"))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Wiley papers\n",
    "# Manual: 111, 114, 116, 119\n",
    "# SET FILE PATH (x3)\n",
    "for f in os.listdir('/Users/auderoy/dsci550/Assignment 2/PDFS'):\n",
    "    count = 0\n",
    "    try:\n",
    "        filename = f.split(\"-\")[3:][0].strip()\n",
    "    except:\n",
    "        filename = f\n",
    "    for c in bik_df[\"Title\"]:\n",
    "        if c[:15] == filename[:15]:\n",
    "            shutil.move(os.path.join('/Users/auderoy/dsci550/Assignment 2/PDFS',f),os.path.join('/Users/auderoy/dsci550/Assignment 2/PDFS',f\"{count}.pdf\"))\n",
    "        else:\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fdc29",
   "metadata": {},
   "source": [
    "# Extract images from PDFs\n",
    "\n",
    "- [The Python Code: Extract PDF images in Python](https://www.thepythoncode.com/article/extract-pdf-images-in-python)\n",
    "- Different [organizational] methods for image generation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images to individual folders\n",
    "\n",
    "def extract_images(df,file,indx):\n",
    "    \n",
    "    print(\"------------------------------\")\n",
    "    print(\"INDEX: \", indx)\n",
    "    \n",
    "    # Open file\n",
    "    pdf_file = fitz.open(file)\n",
    "    \n",
    "    # Create folder for images\n",
    "    folder_location = \"images/extracted_images/\" + df[\"Title\"][indx][:240].replace(\"/\",\"-\") + \"-images\"\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "        \n",
    "    # Iterate over PDF pages\n",
    "    for page_index in range(len(pdf_file)):\n",
    "        # Get page\n",
    "        page = pdf_file[page_index]\n",
    "        image_list = page.get_images()\n",
    "        # Print number of images found on page\n",
    "        if image_list:\n",
    "            print(f\"[+] Found a total of {len(image_list)} images on page {page_index}\")\n",
    "        else:\n",
    "            print(\"[!] No images found on page\", page_index)\n",
    "        for image_index, img in enumerate(page.get_images(), start=1):\n",
    "            try:\n",
    "                # Get image XREF\n",
    "                xref = img[0]\n",
    "                # Extract image bytes\n",
    "                base_image = pdf_file.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                # Get image extension\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                # Load to PIL\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                # Save to local disk\n",
    "                image.save(open(f\"{folder_location}/image{page_index+1}_{image_index}.{image_ext}\", \"wb\"))\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call extract_images for each PDF\n",
    "# PDFs named by index\n",
    "for i in range(0,214):\n",
    "    try:\n",
    "        pdf_file = f\"PDFS/{i}.pdf\"\n",
    "        extract_images(bik_df,pdf_file,i)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a5eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images to one folder\n",
    "\n",
    "def extract_images_new(df,file,indx):\n",
    "    \n",
    "    print(\"------------------------------\")\n",
    "    print(\"INDEX: \", indx)\n",
    "    \n",
    "    # Open file\n",
    "    pdf_file = fitz.open(file)\n",
    "    \n",
    "    # Create folder for images\n",
    "    folder_location = \"images/extracted_images_new\"\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "    \n",
    "    # Iterate over PDF pages\n",
    "    for page_index in range(len(pdf_file)):\n",
    "        # Get page\n",
    "        page = pdf_file[page_index]\n",
    "        image_list = page.get_images()\n",
    "        # Print number of images found on page\n",
    "        if image_list:\n",
    "            print(f\"[+] Found a total of {len(image_list)} images on page {page_index}\")\n",
    "        else:\n",
    "            print(\"[!] No images found on page\", page_index)\n",
    "        for image_index, img in enumerate(page.get_images(), start=1):\n",
    "            try:\n",
    "                # Get image XREF\n",
    "                xref = img[0]\n",
    "                # Extract image bytes\n",
    "                base_image = pdf_file.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                # Get image extension\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                # Load to PIL\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                # Save to local disk\n",
    "                image.save(open(f\"{folder_location}/image{indx}_{page_index+1}_{image_index}.{image_ext}\", \"wb\"))\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9519c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call extract_images_new for each PDF\n",
    "# PDFs named by index\n",
    "for i in range(0,214):\n",
    "    try:\n",
    "        pdf_file = f\"PDFS/{i}.pdf\"\n",
    "        extract_images_new(bik_df,pdf_file,i)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3abfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab446075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images to individual folders\n",
    "\n",
    "def extract_images_jpg(df,file,indx):\n",
    "    \n",
    "    print(\"------------------------------\")\n",
    "    print(\"INDEX: \", indx)\n",
    "    \n",
    "    # Open file\n",
    "    pdf_file = fitz.open(file)\n",
    "    \n",
    "    # Create folder for images\n",
    "    folder_location = \"images/extracted_images_jpg/\" + df[\"Title\"][indx][:240].replace(\"/\",\"-\") + \"-images\"\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "        \n",
    "    # Iterate over PDF pages\n",
    "    for page_index in range(len(pdf_file)):\n",
    "        # Get page\n",
    "        page = pdf_file[page_index]\n",
    "        image_list = page.get_images()\n",
    "        # Print number of images found on page\n",
    "        if image_list:\n",
    "            print(f\"[+] Found a total of {len(image_list)} images on page {page_index}\")\n",
    "        else:\n",
    "            print(\"[!] No images found on page\", page_index)\n",
    "        for image_index, img in enumerate(page.get_images(), start=1):\n",
    "            try:\n",
    "                # Get image XREF\n",
    "                xref = img[0]\n",
    "                # Extract image bytes\n",
    "                base_image = pdf_file.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                # Get image extension\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                # Load to PIL\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                # Save to local disk\n",
    "                image.save(open(f\"{folder_location}/image{page_index+1}_{image_index}.jpg\", \"wb\"))\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call extract_images for each PDF\n",
    "# PDFs named by index\n",
    "for i in range(0,214):\n",
    "    try:\n",
    "        pdf_file = f\"PDFS/{i}.pdf\"\n",
    "        extract_images_jpg(bik_df,pdf_file,i)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50457041",
   "metadata": {},
   "source": [
    "# Grover model\n",
    "\n",
    "- [GitHub: Grover](https://github.com/rowanz/grover) ([demo](https://rowanzellers.com/grover/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ff982",
   "metadata": {},
   "source": [
    "# Generate fake text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18592f55",
   "metadata": {},
   "source": [
    "# Generate fake images\n",
    "\n",
    "- [GitHub: Face Generator](https://github.com/gsurma/face_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export image file names to extracted_images.txt for FaceGeneratorDCGAN.ipynb\n",
    "\n",
    "imagepaths = []\n",
    "\n",
    "for folder in os.listdir('images/extracted_images'):\n",
    "    try:\n",
    "        for image in os.listdir(f'images/extracted_images/{folder}'):\n",
    "            imagepaths.append(f\"{folder}/{image}\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "with open('images/extracted_images.txt', 'w') as f:\n",
    "    for imagepath in imagepaths:\n",
    "        f.write(imagepath)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce56e98",
   "metadata": {},
   "source": [
    "# Generate fake features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd2115",
   "metadata": {},
   "source": [
    "## Author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all authors from all Bik papers\n",
    "authors214 = bik_df[\"Authors\"].tolist()\n",
    "\n",
    "# List of number of authors per paper\n",
    "authors_per_paper = []\n",
    "for i in authors214:\n",
    "    authors_per_paper.append(len(i.split(\",\")))\n",
    "# authors_per_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of author names (string type)\n",
    "def generate_authors():\n",
    "    # for i in range(0,random.randint(3,9)): # Most papers have 3-9 authors\n",
    "    for i in range(0,random.choice(authors_per_paper)): # Weighted random\n",
    "        name = fake.name()\n",
    "        # First author\n",
    "        if i == 0:\n",
    "            authors = name\n",
    "        # Subsequent authors require leading comma\n",
    "        else:\n",
    "            authors = authors + \", \" + name\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da733a88",
   "metadata": {},
   "source": [
    "## Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d388e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample affiliation from Bik papers\n",
    "# Remove empty floats\n",
    "affs214 = bik_df[\"First Author Affiliation\"].tolist()\n",
    "sample_aff = [i for i in affs214 if isinstance(i,str)]\n",
    "\n",
    "# Generate affiliation\n",
    "def generate_aff():\n",
    "    return random.choice(sample_aff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1451303",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_random(df,col_name):\n",
    "    values214 = df[col_name].tolist()\n",
    "    # print(len(values214), values214)\n",
    "    return random.choice(values214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a433a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year\n",
    "\n",
    "def generate_year():\n",
    "    return weighted_random(bik_df,\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742165f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month\n",
    "\n",
    "months214 = bik_df[\"Month\"].tolist()\n",
    "sample_months = [i for i in months214 if i>0] # Remove nan from list\n",
    "\n",
    "def generate_month():\n",
    "    return random.choice(sample_months)\n",
    "\n",
    "months_abc = {1:\"January\",\n",
    "             2:\"February\",\n",
    "             3:\"March\",\n",
    "             4:\"April\",\n",
    "             5:\"May\",\n",
    "             6:\"June\",\n",
    "             7:\"July\",\n",
    "             8:\"August\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec075805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0\n",
    "\n",
    "def generate_0():\n",
    "    return weighted_random(bik_df,\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308dd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "def generate_1():\n",
    "    return weighted_random(bik_df,\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a24f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "def generate_2():\n",
    "    return weighted_random(bik_df,\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "def generate_3():\n",
    "    return weighted_random(bik_df,\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraction\n",
    "\n",
    "def generate_retraction():\n",
    "    return weighted_random(bik_df,\"Retraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "\n",
    "def generate_correction():\n",
    "    return weighted_random(bik_df,\"Correction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ebd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Action\n",
    "\n",
    "def generate_noaction():\n",
    "    return weighted_random(bik_df,\"No Action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Author Career Duration\n",
    "\n",
    "def generate_duration():\n",
    "    return weighted_random(bik_df,\"First Author Career Duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Site\n",
    "\n",
    "def generate_home_site():\n",
    "    return weighted_random(bik_df,\"Home Site\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc1c75",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "affiliations = []\n",
    "year = []\n",
    "month = []\n",
    "month_abc = []\n",
    "zero = []\n",
    "one = []\n",
    "two = []\n",
    "three = []\n",
    "retraction = []\n",
    "correction = []\n",
    "noaction = []\n",
    "duration = []\n",
    "homesite = []\n",
    "title = []\n",
    "\n",
    "for i in range(0,500):\n",
    "    authors.append(generate_authors())\n",
    "    affiliations.append(generate_aff())\n",
    "    year.append(generate_year())\n",
    "    month.append(generate_month())\n",
    "    month_abc.append(months_abc[int(month[i])])\n",
    "    zero.append(generate_0())\n",
    "    one.append(generate_1())\n",
    "    two.append(generate_2())\n",
    "    three.append(generate_3())\n",
    "    retraction.append(generate_retraction())\n",
    "    correction.append(generate_correction())\n",
    "    noaction.append(generate_noaction())\n",
    "    duration.append(generate_duration())\n",
    "    homesite.append(generate_homesite())\n",
    "\n",
    "# Make accessible to LaTeX.ipynb\n",
    "%store authors\n",
    "%store affiliations\n",
    "%store year\n",
    "%store month\n",
    "%store month_abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ded3f2",
   "metadata": {},
   "source": [
    "# Generate 500 full fake papers\n",
    "\n",
    "- [StackExchange: Automatic document generation](https://tex.stackexchange.com/questions/270714/automatic-document-generation-based-on-a-database)\n",
    "- Will run LaTeX generation in separate notebook for organizational purposes (LaTeX/LaTeX.ipynb)\n",
    "- Version in this notebook is note updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c213ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatexContent = '''\n",
    "\\\\documentclass{article}\n",
    "\\\\usepackage[utf8]{inputenc}\n",
    "\\\\usepackage{textalpha}\n",
    "\\\\usepackage{newunicodechar}\n",
    "\\\\newunicodechar{≥}{\\\\ensuremath{\\\\geq}}\n",
    "\\\\usepackage{graphicx}\n",
    "\\\\graphicspath{{./LaTeX/images/}}\n",
    "\n",
    "\\\\title{%(Ti)s}\n",
    "\\\\author{%(Au)s}\n",
    "\\\\date{2020}\n",
    "\n",
    "\\\\begin{document}\n",
    "\n",
    "\\\\maketitle\n",
    "\n",
    "\\\\includegraphics[width=\\\\textwidth]{universe \\\\caption{Testing.}}\n",
    "\n",
    "%(Bo)s\n",
    "\n",
    "\\\\end{document}'''\n",
    "\n",
    "for i in range(0,1):\n",
    "    \n",
    "        ID = str(i + 200)\n",
    "        Author = authors30[i].replace(\",\",\"\\\\and\")\n",
    "        Body = open(f'faketext/{i}.txt', 'r').read().replace(\"\\\\n\",\"\\n\\n\").replace(\"$\",\"\\$\")[1:-2]\n",
    "        Title = Body[:25]\n",
    "\n",
    "        TexFileName = \"LaTeX/\" + ID + '.tex'\n",
    "        TexFile = open(TexFileName,'w')\n",
    "        TexFile.write(LatexContent %{\"Id\" : ID, \"Ti\" : Title, \"Au\" : Author, \"Bo\" : Body })\n",
    "        TexFile.close()\n",
    "        \n",
    "        subprocess.Popen(['pdflatex', TexFileName],shell=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aeadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking that all PDFs are created, no errors\n",
    "\n",
    "count = 0\n",
    "#files = []\n",
    "for i in os.listdir('LaTeX'):\n",
    "    if i[-3:] == \"pdf\":\n",
    "        #files.append(i)\n",
    "        count += 1\n",
    "count\n",
    "#files.sort()\n",
    "#files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a29213c",
   "metadata": {},
   "source": [
    "# Update dataframe, export TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bik_df.to_csv('Bik_v2_updated.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35f655",
   "metadata": {},
   "source": [
    "# Generate fake faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebcaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
